Data Wrangling  refers to The process of transforming raw data into clean, usable form.
its aim to Make data ready for analysis or modeling.
data wrangling also called  data preparation
involves actions such as fix missing values,remove duplicates,correct errors,format data properlyand merge or reshape datasets
There are 5 main steps to data wrangling:
.Discovery: Explore and understand the data to identify its sources, structure, and potential issues.
.Data Cleaning: Correct errors, handle missing values, and remove irrelevant information to ensure data quality.
.Data Transformation: Structure, normalize, and enrich data to align with analysis needs.
.Data Validation: Check the accuracy and consistency of the data with automation in order to ensure it’s reliable.
.Data Publishing: Export the cleaned and validated data in a format that’s ready for analysis, often through dashboards and reports, or further use.





2.  Data scanning refers to the process of examining or exploring data to understand what’s inside.
It aims to Gain an initial understanding before cleaning or analysis use algorithms systematically examine large datasets to identify hidden relationships, trends, anomalies, and statistical structures
involves,looking for patterns,detecting errors,checking distribution of values,understanding data typesand finding anomalies



3. Data filtering refers to selecting specific rows or columns based on conditions with the aim of Reducing data to only what’s relevant for the task.
Key aspects and applications of data filtering include:
Noise Reduction It helps remove unwanted variations, distortions, or "noise" in the data (e.g., removing high-frequency noise from an audio signal or extraneous data in a network scan) to improve clarity and consistency.
Relevance Filtering It isolates only the most useful information for a particular analysis, such as filtering sales data to include only records from a specific time period or region.
Error and Outlier Detection Filtering can be used to identify and exclude erroneous or "bad" observations, duplicates, and outliers that could skew analysis results.
Efficiency Enhancement Working with smaller, filtered datasets reduces computational time and optimizes resource use, leading to more efficient analysis and the development of more accurate AI models.

may include actions such as 
selecting only specific fields (e.g., “name” and “salary”)
remove unwanted categories



